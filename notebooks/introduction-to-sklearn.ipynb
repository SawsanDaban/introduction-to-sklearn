{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SawsanDaban/introduction-to-sklearn/blob/main/notebooks/introduction-to-sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ea2c52-3ead-4484-8a13-59ff459b0f78",
      "metadata": {
        "id": "e3ea2c52-3ead-4484-8a13-59ff459b0f78"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import pickle\n",
        "import requests\n",
        "import tarfile\n",
        "import time\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from sklearn import decomposition, ensemble, linear_model, metrics, tree, utils\n",
        "from sklearn import model_selection, pipeline, preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b241c6-de2e-4c9a-ac4f-6c5a010e2929",
      "metadata": {
        "tags": [],
        "id": "a9b241c6-de2e-4c9a-ac4f-6c5a010e2929"
      },
      "source": [
        "# Working with Real Data\n",
        "\n",
        "When you are just getting started with machine learning it is best to experiment with real-world data (as opposed to artificial data). The following are some good resources of open-source data that you can use for practice or research.\n",
        "\n",
        "* [University of California-Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
        "* [Kaggle](https://www.kaggle.com/datasets),\n",
        "* [OpenDataMonitor](http://opendatamonitor.eu/),\n",
        "* [Wikipedia's list of Machine Learning datasets](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)\n",
        "* [Datasets subreddit](https://www.reddit.com/r/datasets/),\n",
        "* [Quora's list of open datasets](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
        "\n",
        "Major cloud providers all have repositories of publically available datasets.,\n",
        "\n",
        "* [Open Data on AWS](https://registry.opendata.aws/),\n",
        "* [Open Data on GCP](https://cloud.google.com/public-datasets/),\n",
        "* [Open Data on Azure](https://azure.microsoft.com/en-us/services/open-datasets/),\n",
        "    \n",
        "Finally, [Pandas DataReader](https://pydata.github.io/pandas-datareader/) provides a unified API to a [number of datasets](https://pydata.github.io/pandas-datareader/remote_data.html). Note that many of these data sources require you to create an account and get an API key.\n",
        "\n",
        "## MNIST Dataset\n",
        "\n",
        "The original [MNIST](http://yann.lecun.com/exdb/mnist/) dataset consists of 70000 28x28 black and white images in 10 classes. There are 60000 training images and 10000 test images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68d2238-7283-441a-b6d8-4ba3c4c682f0",
      "metadata": {
        "id": "e68d2238-7283-441a-b6d8-4ba3c4c682f0"
      },
      "source": [
        "### Download and extract the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09916110-f2d8-448c-86ee-4be9ae5be169",
      "metadata": {
        "tags": [],
        "id": "09916110-f2d8-448c-86ee-4be9ae5be169"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = pathlib.Path(\"../data/mnist\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "URL = \"https://github.com/davidrpugh/introduction-to-machine-learning/blob/main/data/mnist/mnist.parquet?raw=true\"\n",
        "\n",
        "with open(DATA_DIR / \"mnist.parquet\", 'wb') as f:\n",
        "    response = requests.get(URL)\n",
        "    f.write(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02cc24c9-260e-4840-8872-723f0adc6f41",
      "metadata": {
        "id": "02cc24c9-260e-4840-8872-723f0adc6f41"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "We will load the data using the [Pandas](https://pandas.pydata.org/) library. Highly recommend the most recent edition of [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781491957653/) by Pandas creator Wes Mckinney for anyone interested in learning how to use Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fded7d-02a7-48c3-99e4-d19d001c8ccf",
      "metadata": {
        "id": "c1fded7d-02a7-48c3-99e4-d19d001c8ccf"
      },
      "outputs": [],
      "source": [
        "data = pd.read_parquet(DATA_DIR / \"mnist.parquet\")\n",
        "features = data.drop(\"label\", axis=1)\n",
        "target = data.loc[:, \"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1d7ed7-50ae-46fb-b7c9-cb3cbd5a6f1b",
      "metadata": {
        "id": "de1d7ed7-50ae-46fb-b7c9-cb3cbd5a6f1b"
      },
      "source": [
        "### Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b53a9c4-550d-4577-bf0f-bace74843784",
      "metadata": {
        "id": "9b53a9c4-550d-4577-bf0f-bace74843784"
      },
      "outputs": [],
      "source": [
        "features.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a672145-6e4a-4a8f-9354-773c665e6139",
      "metadata": {
        "id": "5a672145-6e4a-4a8f-9354-773c665e6139"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9991ac-7033-475f-aacc-b0ea0d6dded4",
      "metadata": {
        "id": "ed9991ac-7033-475f-aacc-b0ea0d6dded4"
      },
      "outputs": [],
      "source": [
        "features.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4983c89-dcbf-4403-8d76-7940d95c9122",
      "metadata": {
        "id": "d4983c89-dcbf-4403-8d76-7940d95c9122"
      },
      "outputs": [],
      "source": [
        "features.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a5d4e9-8c96-4da9-b9b7-33a0edbe018e",
      "metadata": {
        "id": "f9a5d4e9-8c96-4da9-b9b7-33a0edbe018e"
      },
      "outputs": [],
      "source": [
        "_ = (target.value_counts()\n",
        "           .sort_index()\n",
        "           .plot(kind=\"bar\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f436f9-7cbb-4cc4-8cbc-ddea21ec9104",
      "metadata": {
        "id": "a4f436f9-7cbb-4cc4-8cbc-ddea21ec9104"
      },
      "source": [
        "### Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13b83e1-6264-4fc3-a3ff-cfc9f493be66",
      "metadata": {
        "id": "b13b83e1-6264-4fc3-a3ff-cfc9f493be66"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(10, 10, sharex=True, sharey=True, figsize=(15, 15))\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        m, _ = features.shape\n",
        "        k = np.random.randint(m)\n",
        "        img = (features.loc[k, :]\n",
        "                       .to_numpy()\n",
        "                       .reshape((28, 28)))\n",
        "        _ = axes[i, j].imshow(img)\n",
        "        _ = axes[i, j].set_title(target.iloc[k])\n",
        "\n",
        "fig.suptitle(\"Random MNIST images\", x=0.5, y=1.0, fontsize=25)\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e76e54f-947c-4123-89c6-e59eedc3621b",
      "metadata": {
        "tags": [],
        "id": "1e76e54f-947c-4123-89c6-e59eedc3621b"
      },
      "source": [
        "# Look at the Big Picture\n",
        "\n",
        "Our goal over these two hands-on workshops will be to build a machine learning modeling pipeline that is capable of classifying images. Today we will mostly focus on classical machine learning algorithms implemented in Scikit-Learn; tomorrow we will revist the same problem using deep learning algorithms implemented in PyTorch. By the time you have finished this two-day workshop you should understand how to build a machine learning application capable of classifying images and be ready to apply what you have learned to a new dataset.\n",
        "\n",
        "This morning we will mostly focus on getting the data and exploring the data to gain new insights. Believe it or not these initial steps are what data scientists and machine learning engineers spend the majority of their time doing! This afternoon we will prepare our data for machine learning, see how to fit a variety of machine learning models to our dataset and shortlist a few candidate models for further analysis. We will then use hyper-parameter tuning to improve the performance of our shortlisted models to arrive at an overall best model. We will finish with a discussion of how to present the results of your model and talk about some of the aspects of deploying a trained model to make predictions.\n",
        "\n",
        "## Framing the problem\n",
        "\n",
        "### What is the business/research objective?\n",
        "\n",
        "Typically building the model is not the overall objective but rather the model itself is one part of a larger process used to answer a business/research question. Knowing the overall objective is important because it will determine your choice of machine learning algorithms to train, your measure(s) of model performance, and how much time you will spend tweaking the hyper-parameters of your model.\n",
        "\n",
        "In our example today, the overall business/research objective is to build a tool for reading electricity meter serial numbers which consist of sequences of sometimes handwritten digits. Part of this tool will be a model that can correctly classify individual handwritten digits. Our image classication model is just one of potentially many other models whose predictions are taken as inputs into another machine learning model that will be used to read off the electricity meter serial numbers. \n",
        "\n",
        "### What is the current solution?\n",
        "\n",
        "Always a good idea to know what the current solution to the problem you are trying to solve. Current solution gives a benchmark for performance. Note that the current \"best\" solution could be very simple or could be very sophisticated. Understanding the current solution helps you think of a good place to start. Example: suppose that the current solution for predicting the price of a house in a given census block is to ignore all the demographic information and predict a simple average of house prices in nearby census blocks. In this case it would probably not make sense to start building a complicated deep learning model to predict housing prices. However, if the current solution was a tuned gradient boosted machine then it probably would not make sense to try a much simpler linear regression model.\n",
        "\n",
        "With all this information, you are now ready to start designing your system. First, you need to frame the problem by answering the following questions.\n",
        "\n",
        "* Is our problem supervised, unsupervised, or reinforcement learning?\n",
        "* Is our problem a classification task, a regression task, or something else? If our problem is a classification task are we trying to classify samples into 2 categories (binary classification) or more than 2 (multi-class classification) categories? If our problem is a regression task, are we trying to predict a single value (univariate regression) or multiple values (multivariate regression) for each sample?\n",
        "* Should you use batch learning or online learning techniques?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52576a2-4c8e-4707-b021-2e424c4633ee",
      "metadata": {
        "id": "a52576a2-4c8e-4707-b021-2e424c4633ee"
      },
      "source": [
        "### Exercise: Selecting a metric\n",
        "\n",
        "Scikit-Learn has a number of different [possible metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) that you can choose from (or you can create your own custom metric if required). Can you find a few metrics that seems appropriate for our image classification model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a01cae0-8437-45c6-8edc-05a51f6a70ae",
      "metadata": {
        "id": "9a01cae0-8437-45c6-8edc-05a51f6a70ae"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a2212c9-f227-4b8f-ac41-6deb6f714976",
      "metadata": {
        "id": "7a2212c9-f227-4b8f-ac41-6deb6f714976"
      },
      "source": [
        "# Creating a Test Dataset\n",
        "\n",
        "Before we look at the data any further, we need to create a test set, put it aside, and never look at it (until we are ready to test our trainined machine learning model!). Why? We don't want our machine learning model to memorize our dataset (this is called overfitting). Instead we want a model that will generalize well (i.e., make good predictions) for inputs that it didn't see during training. To do this we hold split our dataset into training and testing datasets. The training dataset will be used to train our machine learning model(s) and the testing dataset will be used to make a final evaluation of our machine learning model(s).\n",
        "\n",
        "## If you might refresh data in the future...\n",
        "\n",
        "...then you want to use some particular hashing function to compute the hash of a unique identifier for each observation of data and include the observation in the test set if resulting hash value is less than some fixed percentage of the maximum possible hash value for your algorithm. This way even if you fetch more data, your test set will never include data that was previously included in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba37993-2014-4267-9589-fe88d0b21090",
      "metadata": {
        "id": "dba37993-2014-4267-9589-fe88d0b21090"
      },
      "outputs": [],
      "source": [
        "import zlib\n",
        "\n",
        "\n",
        "def in_testing_data(identifier, test_size):\n",
        "    _hash = zlib.crc32(bytes(identifier))\n",
        "    return _hash & 0xffffffff < test_size * 2**32\n",
        "\n",
        "\n",
        "def split_train_test_by_id(data, test_size, id_column):\n",
        "    ids = data[id_column]\n",
        "    in_test_set = ids.apply(lambda identifier: in_testing_data(identifier, test_size))\n",
        "    return data.loc[~in_test_set], data.loc[in_test_set]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c73954-f3a5-464b-b179-17f7fbe142fb",
      "metadata": {
        "id": "b6c73954-f3a5-464b-b179-17f7fbe142fb"
      },
      "source": [
        "## If this is all the data you will ever have...\n",
        "\n",
        "...then you can just set a seed for the random number generator and then randomly split the data. Scikit-Learn has a [`model_selection`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) module that contains tools for splitting datasets into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f791e74d-cddd-4383-8a6c-5b80df4a5daf",
      "metadata": {
        "id": "f791e74d-cddd-4383-8a6c-5b80df4a5daf"
      },
      "outputs": [],
      "source": [
        "model_selection.train_test_split?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b35bf11-0fa5-4879-bbda-e976244ec929",
      "metadata": {
        "id": "6b35bf11-0fa5-4879-bbda-e976244ec929"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "SEED_GENERATOR = np.random.RandomState(SEED)\n",
        "\n",
        "\n",
        "def generate_seed():\n",
        "    return SEED_GENERATOR.randint(np.iinfo(\"uint16\").max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beff5a36-07c7-404f-916b-260c789aff91",
      "metadata": {
        "id": "beff5a36-07c7-404f-916b-260c789aff91"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and testing data\n",
        "_seed = generate_seed()\n",
        "_random_state = np.random.RandomState(_seed)\n",
        "train_features, test_features, train_target, test_target = model_selection.train_test_split(\n",
        "    features,\n",
        "    target,\n",
        "    test_size=1e-1,\n",
        "    random_state=_random_state\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ed741c-996c-4df3-8680-49f14cc0dcf0",
      "metadata": {
        "id": "e9ed741c-996c-4df3-8680-49f14cc0dcf0"
      },
      "outputs": [],
      "source": [
        "train_features.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8115616b-7609-4510-8347-b02e497bed84",
      "metadata": {
        "id": "8115616b-7609-4510-8347-b02e497bed84"
      },
      "outputs": [],
      "source": [
        "train_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3722a295-4c88-4d24-b5cc-b84282f3de57",
      "metadata": {
        "id": "3722a295-4c88-4d24-b5cc-b84282f3de57"
      },
      "outputs": [],
      "source": [
        "train_target.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a75323c-1b45-46b4-80bc-f26b003d50bb",
      "metadata": {
        "id": "8a75323c-1b45-46b4-80bc-f26b003d50bb"
      },
      "outputs": [],
      "source": [
        "train_features.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090af9ae-e36c-4c5d-ac2d-25e1ea1c34df",
      "metadata": {
        "id": "090af9ae-e36c-4c5d-ac2d-25e1ea1c34df"
      },
      "source": [
        "Again, if you want to you can write out the train and test sets to disk to avoid having to recreate them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ede1948-a85a-450f-8c0b-8c18b56ed4d3",
      "metadata": {
        "id": "6ede1948-a85a-450f-8c0b-8c18b56ed4d3"
      },
      "outputs": [],
      "source": [
        "_ = (train_features.join(train_target)\n",
        "                   .to_parquet(DATA_DIR / \"train.parquet\", index=False))\n",
        "\n",
        "_ = (test_features.join(test_target)\n",
        "                   .to_parquet(DATA_DIR / \"test.parquet\", index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4280e020-f990-434c-98b9-361321134432",
      "metadata": {
        "id": "4280e020-f990-434c-98b9-361321134432"
      },
      "source": [
        "# Prepare the data for machine learning algorithms\n",
        "\n",
        "Best practice is to write functions to automate the process of preparing your data for machine learning. Why?\n",
        "\n",
        "* Allows you to reproduce these transformations easily on any dataset.\n",
        "* You will gradually build a library of transformation functions that you can reuse in future projects.\n",
        "* You can use these functions in a live system to transform the new data before feeding it to your algorithms.\n",
        "* This will make it possible for you to easily experiment with various transformations and see which combination of transformations works best.\n",
        "\n",
        "We are working with an benchmark dataset that has already been prepared for analysis (mostly!). You should be aware that academic benchmark datasets are not very representative of the type of datasets that you will encounter in most practical applications. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626fd519-ffb6-4ae9-be55-53d4a54e53c5",
      "metadata": {
        "id": "626fd519-ffb6-4ae9-be55-53d4a54e53c5"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Machine learning algorithms typically don’t perform well when the input numerical attributes have very different scales. The simplest approach is to rescale features so that they all reside within the same range (typically between 0 and 1). This approach is implemented in Scikit-Learn by the [`preprocessing.MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506f4f98-aa73-46b0-82ce-ebf0d4a0ff81",
      "metadata": {
        "id": "506f4f98-aa73-46b0-82ce-ebf0d4a0ff81"
      },
      "outputs": [],
      "source": [
        "preprocessing.MinMaxScaler?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00fe28e-a285-4e87-979e-462be36d0815",
      "metadata": {
        "id": "c00fe28e-a285-4e87-979e-462be36d0815"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters\n",
        "_preprocessing_hyperparameters = {\n",
        "    \"feature_range\": (0, 1),\n",
        "    \"copy\": True,\n",
        "    \"clip\": False,\n",
        "}\n",
        "preprocessor = preprocessing.MinMaxScaler(**_preprocessing_hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ca8616-d29f-4d47-888c-841f6b823b2e",
      "metadata": {
        "id": "33ca8616-d29f-4d47-888c-841f6b823b2e"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_features = preprocessor.fit_transform(train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07170a9-6cb0-4474-8bab-15fec52acc0f",
      "metadata": {
        "id": "e07170a9-6cb0-4474-8bab-15fec52acc0f"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6bf42d-6a72-4b7d-b768-412d5edcb010",
      "metadata": {
        "id": "df6bf42d-6a72-4b7d-b768-412d5edcb010"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_features[:, :5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52de2cc7-960c-43a3-b2de-0a7780b06879",
      "metadata": {
        "id": "52de2cc7-960c-43a3-b2de-0a7780b06879"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_features.min(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034122d4-e0e8-4db9-95b1-0c46b733209d",
      "metadata": {
        "id": "034122d4-e0e8-4db9-95b1-0c46b733209d"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_features.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2306826-abb1-4575-bd53-14f0742131a1",
      "metadata": {
        "id": "d2306826-abb1-4575-bd53-14f0742131a1"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_features.max(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bdd7f6c-0c01-46aa-b2c3-8dce85005f76",
      "metadata": {
        "id": "4bdd7f6c-0c01-46aa-b2c3-8dce85005f76"
      },
      "source": [
        "The `preprocessing.MinMaxScaler` and the `preprocessing.StandardScaler` classes are the first Scikit-Learn `Transformer` classes that we have encountered. As such now is a good to to discuss the Scikit-Learn application programming interface (API). The [Scikit-Learn API](https://scikit-learn.org/stable/modules/classes.html) is one of the best designed API's around and has heavily influenced API design choices of other libraries in the Python Data Science and Machine Learning ecosystem, in particular [Dask](https://dask.org/) and [NVIDIA RAPIDS](https://rapids.ai/). Familiarly with the Scikit-Learn API will make it easier for you to get started with these libraries.\n",
        "\n",
        "The Scikit-Learn API is built around the following key concepts.\n",
        "\n",
        "* Estimators: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an `preprocessing.MinMaxScaler` is an estimator). The estimation itself is performed by the `fit` method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as the `feature_range` parameter in `preprocessing.MinMaxScaler`), and it must be set as an instance variable (generally via a constructor parameter).\n",
        "\n",
        "* Transformers: Some estimators (such as an `preprocessing.MinMaxScaler`) can also transform a dataset; these are called transformers. Once again, the API is simple: the transformation is performed by the transform method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters. All transformers also have a convenience method called `fit_transform` that is equivalent to calling `fit` and then `transform` (but sometimes `fit_transform` is optimized and runs much faster).\n",
        "\n",
        "* Predictors: Finally, some estimators, given a dataset, are capable of making predictions; they are called predictors. A predictor has a `predict` method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a score method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).\n",
        "\n",
        "All of an estimator’s hyperparameters are accessible directly via public instance variables (e.g., `preprocessor.feature_range`), and all the estimator’s learned parameters are accessible via public instance variables with an underscore suffix (e.g., `preprocessor.scale_`). Finally, Scikit-Learn provides reasonable default values for most parameters which makes it easy to quickly create a baseline working system."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6afd0ab6-3d65-466c-b2d6-8926601d4f3d",
      "metadata": {
        "id": "6afd0ab6-3d65-466c-b2d6-8926601d4f3d"
      },
      "source": [
        "### Exercise: MinMaxScaler vs StandardScaler\n",
        "\n",
        "An alternative approach is to rescale features so that they all have zero mean and unit standard deviation. This approach, which is also called standardization, is particularly useful when attributes/features have outliers and when downstream machine learning algorithms assume that attributes/features have a Gaussian or Normal distribution. \n",
        "\n",
        "Create an instance of the [`preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class and use it to rescale the training dataset. Compare the two different rescaled versions of the dataset. Which of the two methods do you prefer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58ce5ba-4568-4f39-a620-93b7fe402396",
      "metadata": {
        "id": "a58ce5ba-4568-4f39-a620-93b7fe402396"
      },
      "outputs": [],
      "source": [
        "# insert your code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d04a639-ba99-47b5-936a-7c6a555c6f08",
      "metadata": {
        "id": "3d04a639-ba99-47b5-936a-7c6a555c6f08"
      },
      "source": [
        "As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c62f5c-21cd-4138-8776-baca812ba3d9",
      "metadata": {
        "id": "54c62f5c-21cd-4138-8776-baca812ba3d9"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Feature engineering is one of the most important parts of any machine learning project. There are two main tasks in feature engineering.\n",
        "\n",
        "* Feature selection: selecting the best subset of features for training. \n",
        "* Feature extraction: combining existing features to produce new features for training.\n",
        "* Feature creation: finding additional data sources to use as features.\n",
        "\n",
        "Feature engineering is often the most labor intensive part of building a machine learning pipeline and often requires extensive expertise/domain knowledge relevant to the problem at hand. Recently packages such as [featuretools](https://www.featuretools.com/) have been developed to (partially) automate the process of feature engineering.\n",
        "\n",
        "The success of deep learning in various domains is in significant part due to the fact that deep learning models are able to automatically engineer features that are most useful for solving certain machine learning tasks. In effect deep learning replaces the expensive to acquire expertise/domain knowledge required to hand-engineer predictive features. \n",
        "\n",
        "A recent example that demonstrates that power of automated feature engineering is [Space2vec](https://medium.com/dessa-news/space-2-vec-fd900f5566), a deep learning based supernovae classifier developed by machine learning engineers with no expertise in Astronomy that was able to outperform the machine learning solution developed by NERSC scientists. The machine learning pipeline developed by NERSC scientists, called [AUTOSCAN](https://portal.nersc.gov/project/dessn/autoscan/), was a significant improvement over the previous solution which relied on manual classification of supernovae by astronomers. However, in order to achieve such high accuracy, the NERSC solution relied on a dataset of hand-engineered features developed by astronomers with over a century of combined training and expertise in the domain. The deep learning algorithm used by space2vec could be applied directly to the raw image data and did not rely on any hand-engineered features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87310b3d-bd8c-47e0-be44-1a7206bcc845",
      "metadata": {
        "id": "87310b3d-bd8c-47e0-be44-1a7206bcc845"
      },
      "source": [
        "### Feature extraction using Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c47345e5-5f5e-4b81-aa4c-c74757d98bb2",
      "metadata": {
        "tags": [],
        "id": "c47345e5-5f5e-4b81-aa4c-c74757d98bb2"
      },
      "outputs": [],
      "source": [
        "decomposition.PCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cffe9c01-03e6-44f4-a803-faf3f939fe9e",
      "metadata": {
        "id": "cffe9c01-03e6-44f4-a803-faf3f939fe9e"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "\n",
        "# hyper-parameters\n",
        "_pca_hyperparameters = {\n",
        "    \"n_components\": 0.95,\n",
        "    \"whiten\": False,\n",
        "}\n",
        "\n",
        "decomposer = decomposition.PCA(**_pca_hyperparameters)\n",
        "engineered_train_features = decomposer.fit_transform(preprocessed_train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afff6d94-d47a-4e8f-b958-490924398545",
      "metadata": {
        "id": "afff6d94-d47a-4e8f-b958-490924398545"
      },
      "outputs": [],
      "source": [
        "engineered_train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b1db02-41f7-4fc3-80c2-4e8e5226c1bf",
      "metadata": {
        "id": "71b1db02-41f7-4fc3-80c2-4e8e5226c1bf"
      },
      "outputs": [],
      "source": [
        "engineered_train_features[:, :5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f166dc3-3c42-470c-aa87-b6cece88fc21",
      "metadata": {
        "id": "4f166dc3-3c42-470c-aa87-b6cece88fc21"
      },
      "outputs": [],
      "source": [
        "engineered_train_features.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40548043-1900-4790-b1f6-06a156383260",
      "metadata": {
        "id": "40548043-1900-4790-b1f6-06a156383260"
      },
      "outputs": [],
      "source": [
        "engineered_train_features.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e6542c-bcdd-425f-8ce6-eea4b3f7bd45",
      "metadata": {
        "id": "91e6542c-bcdd-425f-8ce6-eea4b3f7bd45"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "_ = ax.scatter(\n",
        "    engineered_train_features[:, 0],\n",
        "    engineered_train_features[:, 1],\n",
        "    c=train_target,\n",
        "    alpha=0.05\n",
        ")\n",
        "_ = ax.set_xlabel(\"Component 1\", fontsize=15)\n",
        "_ = ax.set_ylabel(\"Component 2\", fontsize=15)\n",
        "_ = ax.set_title(type(decomposer))\n",
        "_ = ax.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c7ec60-138c-4be1-9934-8eae3089b1f5",
      "metadata": {
        "id": "54c7ec60-138c-4be1-9934-8eae3089b1f5"
      },
      "source": [
        "### Exercise: To whiten, or not to whiten?\n",
        "\n",
        "Take a close look at the doc string for the `decomposition.PCA` algorithm. What happens if you set `n_components` to a number between 0 and 1 (i.e., `n_components=0.95`)? Why might you want to do this? What does setting `whiten=True` do to the output of the algorithm? Re-run the PCA algorithm above setting `whiten=True` to confirm your answer. Why might you want to set `whiten=True`? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a98ccd-0eb1-433f-af72-49d43eb79925",
      "metadata": {
        "id": "39a98ccd-0eb1-433f-af72-49d43eb79925"
      },
      "outputs": [],
      "source": [
        "# insert code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b258fe6-475d-46f1-8349-9be26ae9f70e",
      "metadata": {
        "id": "6b258fe6-475d-46f1-8349-9be26ae9f70e"
      },
      "source": [
        "## Transformation pipelines\n",
        "\n",
        "As you can see creating preprocessing pipelines involves quite a lot of steps and each of the steps needs to be executed in the correct order. Fortunately Scikit-Learn allows you to combine estimators together to create [pipelines](https://scikit-learn.org/stable/modules/compose.html#combining-estimators). We can encapsulate all of the preprocessing logic into instances of the [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) class.\n",
        "\n",
        "The `Pipeline` constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a `fit_transform` method). The names can be anything you like (as long as they are unique). Later we will see how to access the parameters of pipelines using these names when we discuss hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462144af-2e79-4ec8-b0bd-d9c9933ac32f",
      "metadata": {
        "id": "462144af-2e79-4ec8-b0bd-d9c9933ac32f"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "\n",
        "# hyper-parameters\n",
        "_min_max_scaler_hyperparameters = {\n",
        "    \"feature_range\": (0, 1)\n",
        "}\n",
        "\n",
        "_pca_hyperparameters = {\n",
        "    \"n_components\": 154,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "    \"svd_solver\": \"randomized\",\n",
        "    \"whiten\": False,\n",
        "}\n",
        "\n",
        "# default Pipeline constructor\n",
        "preparation_pipeline = pipeline.Pipeline(\n",
        "    [\n",
        "        (\"minmaxscaler\", preprocessing.MinMaxScaler(**_min_max_scaler_hyperparameters)),\n",
        "        (\"pca\", decomposition.PCA(**_pca_hyperparameters)),\n",
        "    ],\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0034f083-3b70-4113-a971-883ee1f5d1ed",
      "metadata": {
        "id": "0034f083-3b70-4113-a971-883ee1f5d1ed"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "\n",
        "# hyper-parameters\n",
        "_min_max_scaler_hyperparameters = {\n",
        "    \"feature_range\": (0, 1)\n",
        "}\n",
        "\n",
        "_pca_hyperparameters = {\n",
        "    \"n_components\": 154,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "    \"svd_solver\": \"randomized\",\n",
        "    \"whiten\": False,\n",
        "}\n",
        "\n",
        "# alternative constructor that is equivalent to the above!\n",
        "preparation_pipeline = pipeline.make_pipeline(\n",
        "    preprocessing.MinMaxScaler(**_min_max_scaler_hyperparameters),\n",
        "    decomposition.PCA(**_pca_hyperparameters),\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c64d85c-11ba-4a3f-86db-0e1eb4d9f8a3",
      "metadata": {
        "id": "7c64d85c-11ba-4a3f-86db-0e1eb4d9f8a3"
      },
      "outputs": [],
      "source": [
        "prepared_train_features = preparation_pipeline.fit_transform(train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c91d3d-f2ea-48a1-b7fa-204bc9f1ddab",
      "metadata": {
        "id": "f3c91d3d-f2ea-48a1-b7fa-204bc9f1ddab"
      },
      "outputs": [],
      "source": [
        "prepared_train_features.min(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6d4c3f-a34f-4f94-8dc6-e833f2902b47",
      "metadata": {
        "id": "bc6d4c3f-a34f-4f94-8dc6-e833f2902b47"
      },
      "outputs": [],
      "source": [
        "prepared_train_features.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20f7251-59d6-4d93-b0fe-97a5f7e1c91f",
      "metadata": {
        "id": "c20f7251-59d6-4d93-b0fe-97a5f7e1c91f"
      },
      "outputs": [],
      "source": [
        "prepared_train_features.max(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c18d39f-153d-4184-800b-7a9161594508",
      "metadata": {
        "id": "2c18d39f-153d-4184-800b-7a9161594508"
      },
      "source": [
        "### Exercise: Feature scaling prior to using PCA?\n",
        "\n",
        "Do you think that it is necessary to rescale the raw features prior to using PCA? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b292be-f7b9-4958-a6d0-c957ebcc50a5",
      "metadata": {
        "id": "01b292be-f7b9-4958-a6d0-c957ebcc50a5"
      },
      "source": [
        "# Select and train a model\n",
        "\n",
        "At last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote transformation pipelines to clean up and prepare your data for machine learning algorithms automatically. You are now ready to select and train a Machine Learning model. You might have been wondering if we were every going to make it to this point! Fact is, most of your time developing machine learning solutions to real-world problems will not be spent training machine learning models: most of your time will be spent preparing the data for machine learning algorithms and most of the computer time will be spent training the machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e627d70a-418f-49ed-8449-5ba412a77d2c",
      "metadata": {
        "id": "e627d70a-418f-49ed-8449-5ba412a77d2c"
      },
      "source": [
        "## Training and evaluating on the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ce4c846-8197-49fd-9391-493cbac70208",
      "metadata": {
        "id": "0ce4c846-8197-49fd-9391-493cbac70208"
      },
      "source": [
        "### Linear models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb069c7-a23b-4d00-9529-25ec8b73ea26",
      "metadata": {
        "id": "8fb069c7-a23b-4d00-9529-25ec8b73ea26"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters\n",
        "_min_max_scaler_hyperparameters = {\n",
        "    \"feature_range\": (0, 1)\n",
        "}\n",
        "\n",
        "preparation_pipeline = pipeline.make_pipeline(\n",
        "    preprocessing.MinMaxScaler(**_min_max_scaler_hyperparameters),\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb0def1-c354-43ed-b464-6630dbb3da85",
      "metadata": {
        "id": "ffb0def1-c354-43ed-b464-6630dbb3da85"
      },
      "outputs": [],
      "source": [
        "prepared_train_features = preparation_pipeline.fit_transform(train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f03c868d-0835-427d-a668-63809b70e65d",
      "metadata": {
        "id": "f03c868d-0835-427d-a668-63809b70e65d"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"fit_intercept\": True,\n",
        "    \"loss\": \"log\",\n",
        "    \"penalty\": None,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "}\n",
        "estimator = linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
        "_ = estimator.fit(prepared_train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc88278-3197-41f0-9600-527206c34c63",
      "metadata": {
        "id": "5bc88278-3197-41f0-9600-527206c34c63"
      },
      "outputs": [],
      "source": [
        "predictions = estimator.predict(prepared_train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e324613b-9ddc-4c2b-b0e7-b0ff34cc3576",
      "metadata": {
        "id": "e324613b-9ddc-4c2b-b0e7-b0ff34cc3576"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada04802-5ae9-4285-bdd5-d8b6ae40057e",
      "metadata": {
        "id": "ada04802-5ae9-4285-bdd5-d8b6ae40057e"
      },
      "source": [
        "Congrats! You have fit your first machine learning model using Scikit-Learn and made some predictions. Now let's see how good those predictions really are."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38833161-7ebe-4b97-8bef-91159c69e939",
      "metadata": {
        "id": "38833161-7ebe-4b97-8bef-91159c69e939"
      },
      "source": [
        "### Evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef1a3be-ada7-4fed-9813-b1158d7aaeab",
      "metadata": {
        "id": "2ef1a3be-ada7-4fed-9813-b1158d7aaeab"
      },
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2caaf7d3-cf20-4008-878e-f57a261db011",
      "metadata": {
        "id": "2caaf7d3-cf20-4008-878e-f57a261db011"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(\n",
        "    train_target,\n",
        "    predictions,\n",
        ")\n",
        "confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a0bf8f-ca7d-43fd-804f-8819fe69c005",
      "metadata": {
        "id": "89a0bf8f-ca7d-43fd-804f-8819fe69c005"
      },
      "outputs": [],
      "source": [
        "# visualize the normalized confusion matrix\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "_normalized_confusion_matrix = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n",
        "np.fill_diagonal(_normalized_confusion_matrix, 0)\n",
        "_ = ax.matshow(_normalized_confusion_matrix)\n",
        "_ = ax.set_xlabel(\"Predicted Class\", fontsize=15)\n",
        "_ = ax.set_ylabel(\"Actual Class\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc0e2e7-4fb3-4cd8-950a-da82c4be4095",
      "metadata": {
        "id": "5dc0e2e7-4fb3-4cd8-950a-da82c4be4095"
      },
      "source": [
        "#### Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2280294c-d524-49fc-bb53-9083662a7fad",
      "metadata": {
        "id": "2280294c-d524-49fc-bb53-9083662a7fad"
      },
      "outputs": [],
      "source": [
        "metrics.precision_score(\n",
        "    train_target,\n",
        "    predictions,\n",
        "    average=\"macro\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbf5770-4c74-4212-b6de-de5637afcba3",
      "metadata": {
        "id": "cfbf5770-4c74-4212-b6de-de5637afcba3"
      },
      "source": [
        "#### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc68c19-f248-4e08-8ab2-565d9aeb3ac5",
      "metadata": {
        "id": "ccc68c19-f248-4e08-8ab2-565d9aeb3ac5"
      },
      "outputs": [],
      "source": [
        "metrics.recall_score(\n",
        "    train_target,\n",
        "    predictions,\n",
        "    average=\"macro\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0065ef0-4bf0-403a-a6a4-219db9a44589",
      "metadata": {
        "id": "c0065ef0-4bf0-403a-a6a4-219db9a44589"
      },
      "source": [
        "#### $F_1$ Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f8aafe-a87c-4e90-8b2f-4ecfbe2d31e8",
      "metadata": {
        "id": "d1f8aafe-a87c-4e90-8b2f-4ecfbe2d31e8"
      },
      "outputs": [],
      "source": [
        "metrics.f1_score(\n",
        "    train_target,\n",
        "    predictions,\n",
        "    average=\"macro\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f89d8a5-caf1-47fc-90af-9896a3767780",
      "metadata": {
        "id": "1f89d8a5-caf1-47fc-90af-9896a3767780"
      },
      "source": [
        "#### Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87fcb8e6-243f-4d1e-be2f-90f03e3a1311",
      "metadata": {
        "id": "87fcb8e6-243f-4d1e-be2f-90f03e3a1311"
      },
      "outputs": [],
      "source": [
        "_scores = estimator.predict_proba(prepared_train_features)\n",
        "metrics.roc_auc_score(\n",
        "    train_target,\n",
        "    _scores,\n",
        "    average=\"macro\",\n",
        "    multi_class=\"ovo\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b434767-0619-42a2-8577-a81c3616d0cd",
      "metadata": {
        "id": "3b434767-0619-42a2-8577-a81c3616d0cd"
      },
      "source": [
        "#### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe04cef-c14c-4735-a7fa-c2131bce5344",
      "metadata": {
        "id": "afe04cef-c14c-4735-a7fa-c2131bce5344"
      },
      "outputs": [],
      "source": [
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "585dc503-a5ce-4f2f-8548-17063a77b826",
      "metadata": {
        "id": "585dc503-a5ce-4f2f-8548-17063a77b826"
      },
      "source": [
        "### Accelerating training\n",
        "\n",
        "How can we speed up training? Use more cores and/or some other [dimensionality reduction](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) technique to reduce the dimension of the feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62eefb30-5bb3-4467-87ba-71f73f708e9b",
      "metadata": {
        "id": "62eefb30-5bb3-4467-87ba-71f73f708e9b"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_pca_hyperparameters = {\n",
        "    \"n_components\": 154,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "    \"svd_solver\": \"randomized\",\n",
        "    \"whiten\": True\n",
        "}\n",
        "\n",
        "# use PCA to reduce dimensionality and standardize features\n",
        "_preparation_pipeline = pipeline.make_pipeline(\n",
        "    decomposition.PCA(**_pca_hyperparameters),\n",
        "    verbose=True,\n",
        ")\n",
        "prepared_train_features = _preparation_pipeline.fit_transform(train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a25e591-5e15-4552-b02d-8fcc811fd8ca",
      "metadata": {
        "id": "2a25e591-5e15-4552-b02d-8fcc811fd8ca"
      },
      "outputs": [],
      "source": [
        "train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8da794-699e-4e45-a6d0-c7e78c37cb36",
      "metadata": {
        "id": "ce8da794-699e-4e45-a6d0-c7e78c37cb36"
      },
      "outputs": [],
      "source": [
        "prepared_train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4a484e-99ee-48ae-9253-fe31e4601f10",
      "metadata": {
        "id": "bc4a484e-99ee-48ae-9253-fe31e4601f10"
      },
      "outputs": [],
      "source": [
        "prepared_train_features.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0030cd86-f8cf-4404-941f-44f2b08f019a",
      "metadata": {
        "id": "0030cd86-f8cf-4404-941f-44f2b08f019a"
      },
      "outputs": [],
      "source": [
        "prepared_train_features.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162da61a-0693-4ee0-b974-bbf14cfb2ffe",
      "metadata": {
        "id": "162da61a-0693-4ee0-b974-bbf14cfb2ffe"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"fit_intercept\": False,\n",
        "    \"loss\": \"log\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"penalty\": None,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "}\n",
        "_estimator = linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
        "_ = _estimator.fit(prepared_train_features, train_target)\n",
        "\n",
        "# make predictions\n",
        "_predictions = _estimator.predict(prepared_train_features)\n",
        "\n",
        "# report the accuracy on the training data\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3110beab-d79a-49dc-b314-ebbb898efb68",
      "metadata": {
        "id": "3110beab-d79a-49dc-b314-ebbb898efb68"
      },
      "source": [
        "### Exercise: experiment with different loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f13789-4069-473d-a3f5-314453430d1a",
      "metadata": {
        "id": "20f13789-4069-473d-a3f5-314453430d1a"
      },
      "outputs": [],
      "source": [
        "linear_model.SGDClassifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53df85d9-c099-404b-8fcb-d9650a7d9d78",
      "metadata": {
        "tags": [],
        "id": "53df85d9-c099-404b-8fcb-d9650a7d9d78"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"fit_intercept\": False,\n",
        "    \"loss\": \"hinge\", # change this!\n",
        "    \"n_jobs\": -1,\n",
        "    \"penalty\": None,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "}\n",
        "_estimator = linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
        "_ = _estimator.fit(prepared_train_features, train_target)\n",
        "\n",
        "# make predictions\n",
        "_predictions = _estimator.predict(prepared_train_features)\n",
        "\n",
        "# report the accuracy on the training data\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c979240-7d8b-4426-80ab-4d6842bdf766",
      "metadata": {
        "id": "7c979240-7d8b-4426-80ab-4d6842bdf766"
      },
      "source": [
        "### Exercise: experiment with different penalties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8441712c-2962-4720-a355-e7d4337e71a3",
      "metadata": {
        "id": "8441712c-2962-4720-a355-e7d4337e71a3"
      },
      "outputs": [],
      "source": [
        "linear_model.SGDClassifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66d6f3e-4613-4d85-bac8-0e6bb929d398",
      "metadata": {
        "id": "b66d6f3e-4613-4d85-bac8-0e6bb929d398"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"alpha\": 1e-4, # try changing this!\n",
        "    \"fit_intercept\": False,\n",
        "    \"l1_ratio\": 0.15, # only used for penalty=elastic_net\n",
        "    \"loss\": \"log\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"penalty\": None, # try changing this!\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "}\n",
        "_estimator = linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
        "_ = _estimator.fit(prepared_train_features, train_target)\n",
        "\n",
        "# make predictions\n",
        "_predictions = _estimator.predict(prepared_train_features)\n",
        "\n",
        "# report the accuracy on the training data\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99063b9-cb93-45cc-b1c7-07abf3cbf3f3",
      "metadata": {
        "id": "e99063b9-cb93-45cc-b1c7-07abf3cbf3f3"
      },
      "source": [
        "### Mini-batch gradient descent\n",
        "\n",
        "Since we talked about the difference between stochastic, batch, and mini-batch gradient descent in the lectures I wanted you to see how to implement mini-batch gradient descent in Scikit-Learn. You will see much more of this idea in the deep learning hands on session so we will not spend too much time on it now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791d5512-923e-4c01-ad58-e28aca768f44",
      "metadata": {
        "tags": [],
        "id": "791d5512-923e-4c01-ad58-e28aca768f44"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_random_state = np.random.RandomState(_seed)\n",
        "\n",
        "n_epochs = 2\n",
        "batch_size = 128\n",
        "X = prepared_train_features\n",
        "y = train_target\n",
        "m, _ = X.shape\n",
        "\n",
        "# define your estimator\n",
        "_classifier_hyperparameters = {\n",
        "    \"alpha\": 1e-4,\n",
        "    \"fit_intercept\": False,\n",
        "    \"l1_ratio\": 0.15,\n",
        "    \"learning_rate\": \"optimal\",\n",
        "    \"loss\": \"log\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"penalty\": None,\n",
        "    \"random_state\": _random_state,\n",
        "    \"warm_start\": True,\n",
        "}\n",
        "estimator = linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
        "\n",
        "# nested for loops implement the training\n",
        "for _ in range(n_epochs):\n",
        "\n",
        "    # shuffle the dataset before every training epoch\n",
        "    shuffled_indices = _random_state.permutation(m)\n",
        "    _X, _y = X[shuffled_indices], y.iloc[shuffled_indices]\n",
        "\n",
        "    for batch_ixs in utils.gen_batches(m, batch_size):\n",
        "        _ = estimator.partial_fit(_X[batch_ixs], _y[batch_ixs], classes=y.unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b83394a-50f1-49e1-9526-8098090b7c9a",
      "metadata": {
        "id": "2b83394a-50f1-49e1-9526-8098090b7c9a"
      },
      "outputs": [],
      "source": [
        "# make predictions\n",
        "_predictions = estimator.predict(prepared_train_features)\n",
        "\n",
        "# report the accuracy on the training data\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dadeb7f8-ee55-4fe6-91f1-4beaf306affc",
      "metadata": {
        "id": "dadeb7f8-ee55-4fe6-91f1-4beaf306affc"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "What is going on here? Are we underfitting? Are we overfitting? If you think that we are underfitting, then what could we do to try and get the model to overfit? If we are overfitting, what could we do to get the model to underfit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1198e099-c5b7-449e-bee4-dc02ba85fa30",
      "metadata": {
        "id": "1198e099-c5b7-449e-bee4-dc02ba85fa30"
      },
      "outputs": [],
      "source": [
        "CV_FOLDS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57558f34-5604-46c3-9b04-050b726a6438",
      "metadata": {
        "tags": [],
        "id": "57558f34-5604-46c3-9b04-050b726a6438"
      },
      "outputs": [],
      "source": [
        "model_selection.learning_curve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76586224-f6e2-4396-975a-5fc55ba523f5",
      "metadata": {
        "id": "76586224-f6e2-4396-975a-5fc55ba523f5"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_random_state = np.random.RandomState(_seed)\n",
        "_classifier_hyperparameters = {\n",
        "    \"fit_intercept\": False,\n",
        "    \"loss\": \"log\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"penalty\": None,\n",
        "    \"random_state\": _random_state,\n",
        "}\n",
        "_estimator = linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
        "_ = _estimator.fit(prepared_train_features, train_target)\n",
        "\n",
        "train_sizes, train_scores, val_scores = model_selection.learning_curve(\n",
        "    _estimator,\n",
        "    prepared_train_features,\n",
        "    train_target,\n",
        "    cv=CV_FOLDS,\n",
        "    n_jobs=-1,\n",
        "    random_state=_random_state,\n",
        "    scoring=\"accuracy\",\n",
        "    train_sizes=np.linspace(0.1, 1.0, 15),\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e2f56d7-1ea7-44e6-8c1b-36a7f0bcdf08",
      "metadata": {
        "id": "4e2f56d7-1ea7-44e6-8c1b-36a7f0bcdf08"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
        "ax.plot(train_sizes, train_scores.mean(axis=1), \"r-+\", linewidth=2, label=\"train\")\n",
        "ax.plot(train_sizes, val_scores.mean(axis=1), \"b-\", linewidth=3, label=\"valid\")\n",
        "ax.set_xlabel(\"Training set size\", fontsize=15)\n",
        "ax.set_ylabel(\"Accuracy\", fontsize=15)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be202dc8-9556-4c87-a70a-06b1df636521",
      "metadata": {
        "id": "be202dc8-9556-4c87-a70a-06b1df636521"
      },
      "source": [
        "### Decision Trees\n",
        "\n",
        "[Decision Trees](https://scikit-learn.org/stable/modules/tree.html) are a non-parametric supervised learning method used for [classification](https://scikit-learn.org/stable/modules/tree.html#tree-classification) and [regression](https://scikit-learn.org/stable/modules/tree.html#tree-regression). The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163cd1e0-6abe-4565-a1a3-1ebc279eb176",
      "metadata": {
        "id": "163cd1e0-6abe-4565-a1a3-1ebc279eb176"
      },
      "outputs": [],
      "source": [
        "tree.DecisionTreeClassifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b62bbb1-9a89-4b3f-93db-f55cb6db0ef8",
      "metadata": {
        "id": "2b62bbb1-9a89-4b3f-93db-f55cb6db0ef8"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "\n",
        "_classifier_hyperparameters = {\n",
        "    \"random_state\": np.random.RandomState(_seed)\n",
        "}\n",
        "\n",
        "estimator = tree.DecisionTreeClassifier(**_classifier_hyperparameters)\n",
        "\n",
        "# here we fit using the raw training features\n",
        "_ = estimator.fit(train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64c2bc4-6ead-401d-9b72-ee84935ccbab",
      "metadata": {
        "id": "b64c2bc4-6ead-401d-9b72-ee84935ccbab"
      },
      "outputs": [],
      "source": [
        "# make predictions\n",
        "_predictions = estimator.predict(train_features)\n",
        "\n",
        "# report the accuracy on the training data\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21482c8-095e-4cc5-92a4-c01f1c61893e",
      "metadata": {
        "id": "e21482c8-095e-4cc5-92a4-c01f1c61893e"
      },
      "source": [
        "Wait, what!? No error at all? Could this model really be absolutely perfect? Unfortunately it is much more likely that the model has badly overfit the training data. How can you be sure? As we saw earlier, you don’t want to touch the testing dataset until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation.\n",
        "\n",
        "The following code use Scikit-Learn [`model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) to randomly split the training set into 5 distinct subsets called folds, then it trains and evaluates our model 5 times, picking a different fold for evaluation every time and training on the other 4 folds. The result is an array containing the 5 evaluation scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51cc464-e680-4933-b477-9aeed953ae3a",
      "metadata": {
        "id": "e51cc464-e680-4933-b477-9aeed953ae3a"
      },
      "outputs": [],
      "source": [
        "estimator_scores = model_selection.cross_val_score(\n",
        "    estimator,\n",
        "    X=train_features,\n",
        "    y=train_target,\n",
        "    cv=CV_FOLDS,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26f0397-ca37-4167-89d7-2af077374cc0",
      "metadata": {
        "id": "b26f0397-ca37-4167-89d7-2af077374cc0"
      },
      "outputs": [],
      "source": [
        "estimator_scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481229f5-1d1a-4e4b-94eb-a7e5c7d8a28b",
      "metadata": {
        "id": "481229f5-1d1a-4e4b-94eb-a7e5c7d8a28b"
      },
      "source": [
        "### Understanding Feature Importance\n",
        "\n",
        "One of the nice features of decision trees is that they provide a way to measure the importance of each of feature. Understanding feature importance is a topic all unto itself. If you are interested in pulling this thread, then I recommend that you start with [SHapley Additive Explanations (SHAP)](https://shap.readthedocs.io/en/latest/index.html) and then take a look through [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d7474a-523f-4af3-b52c-210860f6c83f",
      "metadata": {
        "tags": [],
        "id": "51d7474a-523f-4af3-b52c-210860f6c83f"
      },
      "outputs": [],
      "source": [
        "estimator.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815f85cf-f200-4d6c-b09b-07656b9e8df6",
      "metadata": {
        "id": "815f85cf-f200-4d6c-b09b-07656b9e8df6"
      },
      "outputs": [],
      "source": [
        "is_positive = estimator.feature_importances_ > 0\n",
        "is_positive.sum() / estimator.feature_importances_.size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176e7445-5e4c-4dda-b80e-4b124f840bfb",
      "metadata": {
        "id": "176e7445-5e4c-4dda-b80e-4b124f840bfb"
      },
      "source": [
        "Because our features are pixels we can reshape and plot the features to gain some insight into what might be driving feature importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef5272f2-4e56-4f39-9dbe-82a2cdef218b",
      "metadata": {
        "id": "ef5272f2-4e56-4f39-9dbe-82a2cdef218b"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
        "_average_pixel_importances = (\n",
        "    estimator.feature_importances_\n",
        "             .reshape((1, 28, 28))\n",
        "             .mean(axis=0)\n",
        ")\n",
        "plt.imshow(_average_pixel_importances)\n",
        "plt.title(\"Average Pixel Importance\", fontsize=20)\n",
        "\n",
        "# create a colorbar\n",
        "colorbar = plt.colorbar(ticks=[_average_pixel_importances.min(), _average_pixel_importances.max()])\n",
        "_ = (colorbar.ax\n",
        "             .set_yticklabels([\"Not important\", \"Very important\"], fontsize=15))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a20d47c-5286-4c07-8139-574c3d6be82c",
      "metadata": {
        "id": "0a20d47c-5286-4c07-8139-574c3d6be82c"
      },
      "source": [
        "The majority of features seem to be unimportant which suggests that perhaps we should look at ways to extract the most important features prior to fitting our decision tree model. The code below creates a preprocessing pipeline that uses PCA to reduce the dimension of the feature space and extract more meaningful input features and retrain a decision tree classifier. Does this improve the results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e108e467-36ad-4385-b572-c2f7144274ef",
      "metadata": {
        "tags": [],
        "id": "e108e467-36ad-4385-b572-c2f7144274ef"
      },
      "outputs": [],
      "source": [
        "# use PCA to reduce dimensionality and standardize features\n",
        "_seed = generate_seed()\n",
        "_pca_hyperparameters = {\n",
        "    \"n_components\": 154,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "    \"svd_solver\": \"randomized\",\n",
        "    \"whiten\": False,\n",
        "}\n",
        "_preparation_pipeline = pipeline.make_pipeline(\n",
        "    decomposition.PCA(**_pca_hyperparameters),\n",
        "    verbose=True,\n",
        ")\n",
        "prepared_train_features = _preparation_pipeline.fit_transform(train_features)\n",
        "\n",
        "# fit decision tree classifier\n",
        "estimator = tree.DecisionTreeClassifier(random_state=_random_state,)\n",
        "_ = estimator.fit(prepared_train_features, train_target)\n",
        "\n",
        "# generate a classification report\n",
        "_predictions = estimator.predict(prepared_train_features)\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)\n",
        "\n",
        "# use cross validation to estimate generalization error\n",
        "estimator_scores = model_selection.cross_val_score(\n",
        "    estimator,\n",
        "    X=prepared_train_features,\n",
        "    y=train_target,\n",
        "    cv=CV_FOLDS,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"Average validation error: {estimator_scores.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a03808-6cab-4f4d-89fb-30d001f4f6ab",
      "metadata": {
        "id": "d6a03808-6cab-4f4d-89fb-30d001f4f6ab"
      },
      "source": [
        "### Exercise: Regularizing Decision Trees\n",
        "\n",
        "Our decision tree classifier appears to be overfitting. We need to regularize it. Decreasing `max_*` parameters and increasing `min_*` parameters will increase the amount of regularization applied to the model and will help reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab4c7b2-1fc9-4787-89ec-05e32fa964de",
      "metadata": {
        "tags": [],
        "id": "9ab4c7b2-1fc9-4787-89ec-05e32fa964de"
      },
      "outputs": [],
      "source": [
        "tree.DecisionTreeClassifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2629805-614f-44ff-af20-7a573f35a263",
      "metadata": {
        "id": "b2629805-614f-44ff-af20-7a573f35a263"
      },
      "outputs": [],
      "source": [
        "# fit decision tree classifier\n",
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"criterion\": \"entropy\",\n",
        "    \"max_depth\": 32,\n",
        "    \"max_features\": \"sqrt\",\n",
        "    \"min_samples_leaf\": 1e-3,\n",
        "    \"min_samples_split\": 2e-3,\n",
        "    \"random_state\": np.random.RandomState(_seed)\n",
        "}\n",
        "estimator = tree.DecisionTreeClassifier(**_classifier_hyperparameters)\n",
        "_ = estimator.fit(prepared_train_features, train_target)\n",
        "\n",
        "# make predictions\n",
        "_predictions = estimator.predict(prepared_train_features)\n",
        "\n",
        "# create a classification report\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)\n",
        "\n",
        "# use cross validation to estimate generalization error\n",
        "estimator_scores = model_selection.cross_val_score(\n",
        "    estimator,\n",
        "    X=prepared_train_features,\n",
        "    y=train_target,\n",
        "    cv=CV_FOLDS,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"Average validation error: {estimator_scores.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d3d2251-e5af-43a4-b6e4-4649d41f228a",
      "metadata": {
        "id": "5d3d2251-e5af-43a4-b6e4-4649d41f228a"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Compute the normalized confusion matrix for your decision tree classifier and plot it. Do you notice any patterns? What are the three classes for which your decision tree classifier peforms the worst?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2376ee8-1afe-4b61-a2eb-c43c5bd2bf93",
      "metadata": {
        "id": "d2376ee8-1afe-4b61-a2eb-c43c5bd2bf93"
      },
      "outputs": [],
      "source": [
        "# insert code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a9df98-2d98-42ef-aef6-1aa73321c757",
      "metadata": {
        "id": "45a9df98-2d98-42ef-aef6-1aa73321c757"
      },
      "source": [
        "### Ensemble Methods\n",
        "\n",
        "Building a model on top of many other models is called [ensemble](https://scikit-learn.org/stable/modules/ensemble.html) learning and it is often a great approach to improve the predictions of your machine learning pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc1953f-5ae2-4bb2-b96a-b483a8fc972b",
      "metadata": {
        "id": "cdc1953f-5ae2-4bb2-b96a-b483a8fc972b"
      },
      "source": [
        "#### Random Forests\n",
        "\n",
        "Let’s try the [`ensemble.RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Random forests work by training many decision trees on random subsets of the features, then averaging the predictions made by each of the decision trees to arrive at an overall prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1501b50b-5f60-4f2c-9698-5d517223474e",
      "metadata": {
        "id": "1501b50b-5f60-4f2c-9698-5d517223474e"
      },
      "outputs": [],
      "source": [
        "ensemble.RandomForestClassifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff90a8fe-9956-4e9d-ab44-11d915ff9b84",
      "metadata": {
        "id": "ff90a8fe-9956-4e9d-ab44-11d915ff9b84"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": np.random.RandomState(_seed)\n",
        "}\n",
        "\n",
        "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)\n",
        "_ = estimator.fit(train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053b4fc9-d9fb-47de-a85e-0f1af0e62637",
      "metadata": {
        "id": "053b4fc9-d9fb-47de-a85e-0f1af0e62637"
      },
      "outputs": [],
      "source": [
        "# make predictions\n",
        "_predictions = estimator.predict(train_features)\n",
        "\n",
        "# generate a classification report\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ad4c91-d869-434f-b316-3aa66bcc06b0",
      "metadata": {
        "id": "d5ad4c91-d869-434f-b316-3aa66bcc06b0"
      },
      "source": [
        "##### Visualizing Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9091f64-c2fa-43da-94f3-3c482342d382",
      "metadata": {
        "id": "f9091f64-c2fa-43da-94f3-3c482342d382"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
        "_average_pixel_importances = (\n",
        "    estimator.feature_importances_\n",
        "             .reshape((1, 28, 28))\n",
        "             .mean(axis=0)\n",
        ")\n",
        "plt.imshow(_average_pixel_importances)\n",
        "plt.title(\"Average Pixel Importance\", fontsize=20)\n",
        "\n",
        "# create a colorbar\n",
        "colorbar = plt.colorbar(ticks=[_average_pixel_importances.min(), _average_pixel_importances.max()])\n",
        "_ = (colorbar.ax\n",
        "             .set_yticklabels([\"Not important\", \"Very important\"], fontsize=15))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5b5cc1-598d-468e-9762-848f3c116a9b",
      "metadata": {
        "id": "4b5b5cc1-598d-468e-9762-848f3c116a9b"
      },
      "source": [
        "##### Measuring Validation Error\n",
        "\n",
        "Again you can use $k$-fold CV to estimate the validation error for your random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f595a6ef-86ac-4f1b-9a43-cc69449a82cb",
      "metadata": {
        "id": "f595a6ef-86ac-4f1b-9a43-cc69449a82cb"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": np.random.RandomState(_seed)\n",
        "}\n",
        "\n",
        "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)\n",
        "\n",
        "estimator_scores = model_selection.cross_val_score(\n",
        "    estimator,\n",
        "    X=train_features,\n",
        "    y=train_target,\n",
        "    cv=CV_FOLDS,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b5d59d-80ea-4e25-bee1-76c1f59647f8",
      "metadata": {
        "id": "44b5d59d-80ea-4e25-bee1-76c1f59647f8"
      },
      "outputs": [],
      "source": [
        "estimator_scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e651ae-38cb-49ea-b6b7-6368a959d2aa",
      "metadata": {
        "id": "49e651ae-38cb-49ea-b6b7-6368a959d2aa"
      },
      "source": [
        "Alternatively, with random forests, you can avoid CV altogether by only using a random subset of training samples to fit each tree. The unused training samples can then be used to estimate the validation error for each tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4101ea46-ab9b-4126-96a1-9ddc8b7ed3cc",
      "metadata": {
        "id": "4101ea46-ab9b-4126-96a1-9ddc8b7ed3cc"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"bootstrap\": True,\n",
        "    \"max_samples\": 0.9,\n",
        "    \"n_jobs\": -1,\n",
        "    \"oob_score\": True,\n",
        "    \"random_state\": np.random.RandomState(_seed)\n",
        "}\n",
        "\n",
        "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)\n",
        "_ = estimator.fit(train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b2a19b-5eff-4df7-94a5-efcaed4324af",
      "metadata": {
        "id": "44b2a19b-5eff-4df7-94a5-efcaed4324af"
      },
      "outputs": [],
      "source": [
        "estimator.oob_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e02dfd-16b7-4ad2-ae94-f5192b508335",
      "metadata": {
        "id": "04e02dfd-16b7-4ad2-ae94-f5192b508335"
      },
      "source": [
        "### Exercise: Regularizing Random Forests\n",
        "\n",
        "Our random forest classifier is pretty good. Can tune the behavior of each tree using the same tuning parameters as above. Also can control the number of estimators used in constructing the ensemble: more estimators means a more flexible model. The Scikit Learn documentation has a good discussion on [parameter tunning strategies](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters) for random forest classifiers and regressors. Manually tune the hyperparameters of the random forest classifier to train and find a good set of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98600105-39e4-4386-97b7-35ab98d7e127",
      "metadata": {
        "id": "98600105-39e4-4386-97b7-35ab98d7e127"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"criterion\": \"entropy\",\n",
        "    \"max_depth\": 16,\n",
        "    \"max_features\": \"sqrt\",\n",
        "    \"max_samples\": 0.9,\n",
        "    \"min_samples_leaf\": 1e-3,\n",
        "    \"min_samples_split\": 2e-3,\n",
        "    \"n_estimators\": 250,\n",
        "    \"bootstrap\": True,\n",
        "    \"max_samples\": 0.9,\n",
        "    \"n_jobs\": -1,\n",
        "    \"oob_score\": True,\n",
        "    \"random_state\": np.random.RandomState(_seed)\n",
        "}\n",
        "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)\n",
        "_ = estimator.fit(train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff10a21d-3a58-48b6-a9a0-8966788b50b6",
      "metadata": {
        "id": "ff10a21d-3a58-48b6-a9a0-8966788b50b6"
      },
      "outputs": [],
      "source": [
        "# create a classification report\n",
        "_predictions = estimator.predict(train_features)\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c918b9e6-57c7-41ed-8179-732b8c9c84c7",
      "metadata": {
        "id": "c918b9e6-57c7-41ed-8179-732b8c9c84c7"
      },
      "outputs": [],
      "source": [
        "estimator.oob_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f616a7a-5c07-405e-ad8b-32dba5be8bc3",
      "metadata": {
        "id": "2f616a7a-5c07-405e-ad8b-32dba5be8bc3"
      },
      "source": [
        "### Exercise: Exploring Gradient Boosted Trees\n",
        "\n",
        "Read the docs for to understand default behavior of the [`ensemble.HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). Fit a gradient boosted classifier and manually tune the hyperparameters and see if you can outperform your random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d1513c-5972-4f1d-909e-a58bd9be3ff2",
      "metadata": {
        "tags": [],
        "id": "e0d1513c-5972-4f1d-909e-a58bd9be3ff2"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_classifier_hyperparameters = {\n",
        "    \"max_iter\": 100,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "    \"scoring\": \"accuracy\",\n",
        "    \"tol\": 1e-3\n",
        "}\n",
        "\n",
        "estimator = ensemble.HistGradientBoostingClassifier(**_classifier_hyperparameters)\n",
        "_ = estimator.fit(prepared_train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a5f724-e8cb-45d1-84a5-c09ac943b162",
      "metadata": {
        "id": "f7a5f724-e8cb-45d1-84a5-c09ac943b162"
      },
      "outputs": [],
      "source": [
        "# make predictions\n",
        "_predictions = estimator.predict(prepared_train_features)\n",
        "\n",
        "# generate a classification report\n",
        "_report = metrics.classification_report(\n",
        "    train_target,\n",
        "    _predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016de61c-6c21-40c4-9c2e-9aed6f33030e",
      "metadata": {
        "tags": [],
        "id": "016de61c-6c21-40c4-9c2e-9aed6f33030e"
      },
      "outputs": [],
      "source": [
        "estimator_scores = model_selection.cross_val_score(\n",
        "    estimator,\n",
        "    X=prepared_train_features,\n",
        "    y=train_target,\n",
        "    cv=CV_FOLDS,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b4689c-28b4-42d5-b026-f207d8e2490a",
      "metadata": {
        "id": "02b4689c-28b4-42d5-b026-f207d8e2490a"
      },
      "outputs": [],
      "source": [
        "estimator_scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4480006-5cd1-467d-bc1d-be8a52d7abd6",
      "metadata": {
        "id": "b4480006-5cd1-467d-bc1d-be8a52d7abd6"
      },
      "source": [
        "# Fine-tune your models\n",
        "\n",
        "Most common approach to tuning a model is to manually fiddle with the hyperparameters until you find a great combination of hyperparameter values. Needless to day, this approach to model tuning is very tedious and not at all scientific. We can do much better!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09227c31-3011-4e3e-b024-7e4cc7468093",
      "metadata": {
        "id": "09227c31-3011-4e3e-b024-7e4cc7468093"
      },
      "source": [
        "## Grid Search\n",
        "\n",
        "Simplest approach is to use Scikit-Learn’s [`model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out. The `model_selection.GridSearchCV` class will then use cross-validation to evaluate all the possible combinations of hyperparameter values and return the best scoring set of hyperparameters according to your specified metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536c7ffa-c6b6-4003-a1a0-57abc5d27080",
      "metadata": {
        "id": "536c7ffa-c6b6-4003-a1a0-57abc5d27080"
      },
      "outputs": [],
      "source": [
        "# hyperparameters that you don't want to tune\n",
        "_seed = generate_seed()\n",
        "_pca_default_hyperparameters = {\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "    \"svd_solver\": \"randomized\",\n",
        "    \"whiten\": False,\n",
        "}\n",
        "\n",
        "_seed = generate_seed()\n",
        "_classifier_default_hyperparameters = {\n",
        "    \"bootstrap\": False,\n",
        "    \"oob_score\": False,\n",
        "    \"max_samples\": None,\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "}\n",
        "\n",
        "_pipeline = pipeline.make_pipeline(\n",
        "    decomposition.PCA(\n",
        "        **_pca_default_hyperparameters\n",
        "    ),\n",
        "    ensemble.RandomForestClassifier(\n",
        "        **_classifier_default_hyperparameters\n",
        "    ),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "_parameter_grid = [\n",
        "    { # small number of deep trees\n",
        "        \"pca__n_components\": [25],\n",
        "        \"randomforestclassifier__max_depth\": [None],\n",
        "        \"randomforestclassifier__max_features\": [None],\n",
        "        \"randomforestclassifier__n_estimators\": [2, 4, 8, 16],\n",
        "        \"randomforestclassifier__min_samples_leaf\": [1],\n",
        "        \"randomforestclassifier__min_samples_split\": [2]\n",
        "    }, # 1 * 1 * 1 * 4 * 1 * 1 = 4 parameter combinations to try\n",
        "    { # large number of short trees\n",
        "        \"pca__n_components\": [25],\n",
        "        \"randomforestclassifier__max_depth\": [None],\n",
        "        \"randomforestclassifier__max_features\": [\"log2\"],\n",
        "        \"randomforestclassifier__n_estimators\": [100, 200, 400, 800],\n",
        "        \"randomforestclassifier__min_samples_leaf\": [10, 100, 1000],\n",
        "    } # 1 * 1 * 1 * 4 * 3 = 12 parameter combinations to try\n",
        "] # 4 + 12 = 16 total parameter combinations to try\n",
        "\n",
        "estimator = model_selection.GridSearchCV(\n",
        "    _pipeline,\n",
        "    _parameter_grid,\n",
        "    cv=CV_FOLDS, # 3 * 16 = 48 total fits!\n",
        "    scoring=\"accuracy\",\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1,\n",
        "    pre_dispatch=2, # important to set this properly to avoid OOM errors\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b40d47f2-a368-44cb-9c28-50667776077d",
      "metadata": {
        "tags": [],
        "id": "b40d47f2-a368-44cb-9c28-50667776077d"
      },
      "outputs": [],
      "source": [
        "_ = estimator.fit(train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39895ae-2739-44fc-9b9e-a8456f5c667f",
      "metadata": {
        "id": "f39895ae-2739-44fc-9b9e-a8456f5c667f"
      },
      "outputs": [],
      "source": [
        "estimator.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28c7059-dec2-4f07-b2fb-058e860025a5",
      "metadata": {
        "id": "b28c7059-dec2-4f07-b2fb-058e860025a5"
      },
      "outputs": [],
      "source": [
        "estimator.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d56606c-1216-41e8-9f49-4c8afaa86cb7",
      "metadata": {
        "id": "6d56606c-1216-41e8-9f49-4c8afaa86cb7"
      },
      "outputs": [],
      "source": [
        "estimator.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb75865-e210-406e-9902-705fa06b52c1",
      "metadata": {
        "id": "3eb75865-e210-406e-9902-705fa06b52c1"
      },
      "source": [
        "You should save every model you experiment with so that you can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to more easily compare scores across model types and compare the types of errors they make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7b63ee-6d09-4de9-8a7b-26bf301cde76",
      "metadata": {
        "id": "7f7b63ee-6d09-4de9-8a7b-26bf301cde76"
      },
      "outputs": [],
      "source": [
        "RESULTS_DIR = pathlib.Path(\"../results\")\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "_ = joblib.dump(estimator, RESULTS_DIR / f\"grid-search-cv-classifier-{timestamp}.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe59c5d-9048-4385-9766-874f332e44db",
      "metadata": {
        "id": "3fe59c5d-9048-4385-9766-874f332e44db"
      },
      "source": [
        "For reference here is how you would reload the trained model from the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "846db76e-d988-48ab-b43a-8329f3f889f9",
      "metadata": {
        "id": "846db76e-d988-48ab-b43a-8329f3f889f9"
      },
      "outputs": [],
      "source": [
        "reloaded_estimator = joblib.load(RESULTS_DIR / f\"grid-search-cv-classifier-{timestamp}.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db94fb6-e955-4edc-95d0-edfa55d6bf71",
      "metadata": {
        "id": "0db94fb6-e955-4edc-95d0-edfa55d6bf71"
      },
      "outputs": [],
      "source": [
        "reloaded_estimator.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915154a7-3da0-4e9c-b6a7-673d34e8a736",
      "metadata": {
        "id": "915154a7-3da0-4e9c-b6a7-673d34e8a736"
      },
      "source": [
        "## Randomized Search\n",
        "\n",
        "The grid search approach is fine when you are exploring relatively few combinations but when the hyperparameter search space is large it is often preferable to use [`model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) instead. Instead of trying out all possible combinations, `model_selection.RandomizedSearchCV` evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits.\n",
        "\n",
        "* More efficient exploration of the hyperparameter space.\n",
        "* More control over the computing budget you want to allocate to hyperparameter search.\n",
        "\n",
        "Cost is that it requires some fairly detailed knowledge of probability distributions to implement as you need to choose probability distributions for each hyperparameter that you wish to tune."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "676997a0-f16b-4fa6-92d2-c2019dde679a",
      "metadata": {
        "id": "676997a0-f16b-4fa6-92d2-c2019dde679a"
      },
      "source": [
        "### Geometric Discrete Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de1416d-d2aa-4804-8e9a-9b1dd0191ecd",
      "metadata": {
        "id": "1de1416d-d2aa-4804-8e9a-9b1dd0191ecd"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "x = np.arange(1, 1000, 1)\n",
        "y = (stats.geom\n",
        "          .pmf(x, p=0.005))\n",
        "ax.plot(x, y, 'o', alpha=0.05)\n",
        "ax.set_xscale(\"log\")\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"Probability Mass\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7547001-a326-4185-ae08-d452b5183be2",
      "metadata": {
        "id": "f7547001-a326-4185-ae08-d452b5183be2"
      },
      "outputs": [],
      "source": [
        "(stats.geom(p=0.005)\n",
        "      .rvs(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e15275-6634-483c-98cf-9e26f255e328",
      "metadata": {
        "id": "65e15275-6634-483c-98cf-9e26f255e328"
      },
      "source": [
        "### Beta Continuous Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048a4771-1252-4528-9c79-80f08c445810",
      "metadata": {
        "id": "048a4771-1252-4528-9c79-80f08c445810"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "x = np.linspace(0, 1, 1000)\n",
        "y = (stats.beta\n",
        "          .pdf(x, a=25, b=2))\n",
        "ax.plot(x, y)\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"Probability Mass\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfce8094-c6b2-4176-86a3-d015fb52c60b",
      "metadata": {
        "id": "bfce8094-c6b2-4176-86a3-d015fb52c60b"
      },
      "outputs": [],
      "source": [
        "(stats.beta(a=100, b=10)\n",
        "      .rvs(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce24cbfc-7734-40e4-8892-b542f1a98efc",
      "metadata": {
        "id": "ce24cbfc-7734-40e4-8892-b542f1a98efc"
      },
      "source": [
        "### Exercise: Playing with probability distributions\n",
        "\n",
        "Play around with the parameters of the distributions above and see if you can get a feel for how the shape of the pmf/pdf changes with different parameter values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c5716f6-e9a9-49cf-93c5-81d18a71fb53",
      "metadata": {
        "id": "5c5716f6-e9a9-49cf-93c5-81d18a71fb53"
      },
      "outputs": [],
      "source": [
        "_seed = generate_seed()\n",
        "_pca_default_hyperparameters = {\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "    \"whiten\": False,\n",
        "}\n",
        "\n",
        "_seed = generate_seed()\n",
        "_classifier_default_hyperparameters = {\n",
        "    \"bootstrap\": False,\n",
        "    \"oob_score\": False,\n",
        "    \"max_samples\": None,\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": np.random.RandomState(_seed),\n",
        "}\n",
        "\n",
        "_pipeline = pipeline.make_pipeline(\n",
        "    decomposition.PCA(\n",
        "        **_pca_default_hyperparameters\n",
        "    ),\n",
        "    ensemble.RandomForestClassifier(\n",
        "        **_classifier_default_hyperparameters\n",
        "    ),\n",
        "    verbose=True,\n",
        "    memory=\"../results/models\", # enables on-disk caching!\n",
        ")\n",
        "\n",
        "\n",
        "_parameter_distributions = [\n",
        "    {\n",
        "        \"pca__n_components\": stats.beta(a=100, b=10),\n",
        "        \"randomforestclassifier__n_estimators\": stats.geom(p=0.005),\n",
        "        \"randomforestclassifier__min_samples_split\": stats.beta(a=2, b=100),\n",
        "    }\n",
        "]\n",
        "\n",
        "_seed = generate_seed()\n",
        "estimator = model_selection.RandomizedSearchCV(\n",
        "    _pipeline,\n",
        "    _parameter_distributions,\n",
        "    cv=CV_FOLDS,\n",
        "    n_iter=10, # 3 * 10 = 30 total fits!\n",
        "    n_jobs=-1,\n",
        "    pre_dispatch=2, # important to set this properly to avoid OOM errors\n",
        "    random_state=np.random.RandomState(_seed),\n",
        "    scoring=\"accuracy\",\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94002a67-8041-4321-bd67-dd34ea02d44a",
      "metadata": {
        "id": "94002a67-8041-4321-bd67-dd34ea02d44a"
      },
      "outputs": [],
      "source": [
        "_ = estimator.fit(train_features, train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60942518-a452-4fe9-9a8e-f451e7319dbd",
      "metadata": {
        "id": "60942518-a452-4fe9-9a8e-f451e7319dbd"
      },
      "outputs": [],
      "source": [
        "estimator.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5973c6-38bf-4e23-af78-a44f3b475245",
      "metadata": {
        "id": "cc5973c6-38bf-4e23-af78-a44f3b475245"
      },
      "outputs": [],
      "source": [
        "estimator.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34250b6-57b3-4d9b-a6ec-e061bb060241",
      "metadata": {
        "id": "f34250b6-57b3-4d9b-a6ec-e061bb060241"
      },
      "outputs": [],
      "source": [
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "_ = joblib.dump(estimator, RESULTS_DIR / f\"randomized-search-cv-classifier-{timestamp}.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d930c4-e117-4b17-a491-b5e29428c69c",
      "metadata": {
        "id": "04d930c4-e117-4b17-a491-b5e29428c69c"
      },
      "source": [
        "### Exercise:\n",
        "\n",
        "Fine-tune one of your models using Grid Search; fine-tune another model using Randomized Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76d6d00-4881-4def-bf0a-88219991d5a4",
      "metadata": {
        "id": "f76d6d00-4881-4def-bf0a-88219991d5a4"
      },
      "outputs": [],
      "source": [
        "# insert your code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe7255e-4f60-469c-b541-f6935d9cb6e1",
      "metadata": {
        "id": "efe7255e-4f60-469c-b541-f6935d9cb6e1"
      },
      "source": [
        "# Evaluate your models on the test dataset\n",
        "\n",
        "After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a73ca70-1d2b-4053-b500-5d0be0c19f62",
      "metadata": {
        "id": "8a73ca70-1d2b-4053-b500-5d0be0c19f62"
      },
      "outputs": [],
      "source": [
        "# make predictions\n",
        "predictions = estimator.predict(test_features)\n",
        "\n",
        "# generate a classification report\n",
        "_report = metrics.classification_report(\n",
        "    test_target,\n",
        "    predictions,\n",
        ")\n",
        "print(_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55224491-47cc-443d-8592-65fd362e4128",
      "metadata": {
        "id": "55224491-47cc-443d-8592-65fd362e4128"
      },
      "source": [
        "If you did a lot of hyperparameter tuning, the performance will usually be slightly worse than what you measured using cross-validation (because your system ends up fine-tuned to perform well on the validation data and will likely not perform as well on unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2cedef-b497-4eda-9bbd-ab04263e2087",
      "metadata": {
        "id": "ed2cedef-b497-4eda-9bbd-ab04263e2087"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}